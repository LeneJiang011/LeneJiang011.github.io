<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head><meta name="generator" content="Hexo 3.8.0">

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!--Description-->
    
        <meta name="description" content="Figure 1. An example of a successful catch.
ClaimWe propose by introducing virtual reality (VR) into the throw-and-catch interaction, users who are af">
    

    <!--Author-->
    
        <meta name="author" content="Shixin Jiang">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="Virtually Catch A Real Ball">
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="Jiang&#39;s Homepage">

    <!--Page Cover-->
    
        <meta property="og:image" content="">
    

    <!-- Title -->
    
    <title>Virtually Catch A Real Ball - Jiang&#39;s Homepage</title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/sass/main.css">

    <!--[if lt IE 8]>
        <script src="/js/ie/html5shiv.js"></script>
    <![endif]-->

    <!--[if lt IE 8]>
        <link rel="stylesheet" href="/sass/ie8.css">
    <![endif]-->

    <!--[if lt IE 9]>
        <link rel="stylesheet" href="/sass/ie9.css">
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdn.rawgit.com/noelboss/featherlight/1.3.5/release/featherlight.min.css" type="text/css" rel="stylesheet">

    <!-- Google Analytics -->
    


</head>

<body>

    <div id="wrapper">

        <!-- Menu -->
        <!-- Header -->
<header id="header">
    <div class="inner">

        <!-- Logo -->
        <a href="/" class="logo">
            <span class="symbol"><img src="/images/logo.svg" alt=""></span><span class="title">Jiang's Homepage</span>
        </a>

        <!-- Nav -->
        <nav>
            <ul>
                <li><a href="#menu">Menu</a></li>
            </ul>
        </nav>

    </div>
</header>

<!-- Menu -->
<nav id="menu">
    <h2>Menu</h2>
    <ul>
        
            <li>
                <a href="/">Home</a>
            </li>
        
            <li>
                <a href="/archives">Archives</a>
            </li>
        
            <li>
                <a href="/about">Profile</a>
            </li>
        
    </ul>
</nav>


        <div id="main">
            <div class="inner">

                <!-- Main Content -->
                

    <h1>Virtually Catch A Real Ball</h1>



<!-- Gallery -->


<!-- Content -->
<p><img src="/images/catch.png" alt="A successful catch" width="50%" height="50%" center=""><br>Figure 1. An example of a successful catch.</p>
<p><strong>Claim</strong><br>We propose by introducing virtual reality (VR) into the throw-and-catch interaction, users who are afraid of failing to catch the ball can act more proactively. Moreover, while getting back to the fully real interaction, users can be able to catch the ball with more confidence.</p>
<p><strong>Status: Suspended</strong><br>We presented a simple demo at the coursework presentation.<br>However, due to technical limitations, we failed to draw the full picture of this project.<br>That is also the reason why we failed to win resources and support. As a result, this project has been suspended since 2019.</p>
<hr>
<p><strong>INTRODUCTION</strong><br>Inspired by user feedback from the previous <a href="http://lenejiang.com/2021/02/04/MTL/">project</a>, which decribes that users felt less frustrated when they failed in VR, we propose to introduce VR into the throw-and-catch training.<br>Referring to Figure 1, the user is wearing a headset, so what he sees is fully virtual. But at the same time, he catches a real object - the ball.<br>Here we present a trial operation.</p>
<p><video width="50%" height="400" controls><br>    <source src="/images/beat.mp4" type="video/mp4"><br></video><br>We suppose that by training that way, users who are afraid of missing the ball can act more proactively. For example, users who are afraid of missing the ball might hesitate to reach the ball in the real world; Whereas in VR training, since users are less likely to be frustrated by their failures, we expect that users can reach out their hands/arms more actively in VR.<br>We also expect that abilities contrianed by the fear emontion can be released through this kind of trainig. So that through this training, users can catch the ball with more confidence, showing better performance. </p>
<p><strong>SYSTEM</strong><br>Due to technical limitations, the final system was fully bulit on HTC VIVE and its portable trackers.<br>However, in the whole process of realizing proposed ideas, we experimented more approaches, and experienced a lot of trial and error.<br>In the following I am going to present how we experimented and figured out the implementation.<br><strong><em>Why not using fully-virtual ball?</em></strong><br>The reason is that at that time, hand tracking technologies was not integrated into VR headset. Since our target was throw-and-catch interaction, the lack of hand tracking would make the whole process non-interactive.<br>Though combining Leap Motion with Oculus Go seems like a way out, we experimented and found that the headset and the controller can not be separated. To be more specifically, the thrower holding the controller (to trigger input) should stand within 1-meter distance from the catcher wearing the headset. Otherwise, the tracking is unstable and the throw-and-catch interaction is impossible.<br>Therefore, we took a step forward. We decided to track a real ball. A real ball can also provide the catcher with feedback, and we considered that as an advantage because we could provide the catcher with rewards.<br><strong><em>Why not using the real ball?</em></strong><br>The reason is that due to our limited abilities, tracking and rendering the flying ball at realtime was impossible.<br>We tried using OptiTrack to track the ball. However, the ball is too small to set trackers on. So we painted the ball with refractive material to make it detectable. This approach failed because the processing of detected data could not catch up with real time. In other words, the rendered virtual ball was always later than the real ball.<br>We also tried using machine learning technologies to predict the ball’s trajectory. One of our team members is a professional baseball pitcher. His professional control over the ball’s delivery speed and trajectory made this approach possible. W e propose by using the ball’s early flight information, we can predict the ball’s later fligh information thus making the rendering of the virtual ball catch up with the real ball. The figure below shows how we collected the ball’s flight information. We did not have access to depth camera at that time. So we used a sliding bed to integrate change in one axis, and our pitcher tried to throw the ball along the slope. In that way, we could get an approximate 3D position of the ball.<br>As you may expected, the prediction was inaccurate because errors had already been integrated in the detection. The rendered virtual ball finally failed to catch up with the real one, so we turned our minds to using pairing trackers of HTC VIVE.<br><img src="/images/pitch.png" alt="The throw" width="25%" height="50%" center=""><br>Figure 2. Approximate the ball’s 3D position.<br><strong><em>So we made a ball using the HTC tracker</em></strong><br>We decided to render the ball on the HTC trackers. However, the shape of the tracker is not at all like a ball. So we designed a ball-like 3D model, printed out it, and set the HTC tracker on it. The designed model is shown in the figure below. The black part is the model of the HTC VIVE. The blue part is what we designed to make the whole shape like a ball.<br><img src="/images/ball.png" alt="The virtual ball" width="40%" height="50%" center=""><br>Figure 3. The designed model.<br>However…the 3D printer crashed three days before the deadline!!!<br>We tried to repair it. We repaired it on the final day. But the required time for printing the whole model was too long…so we stopped it to catch up with the demo presentation.<br>And here it goes…<br><img src="/images/realBall.png" alt="The virtual ball" width="40%" height="50%" center=""><br>Figure 4. The final ball.<br>This is what you see in the very beginning of this post, also the one you see in the video.</p>
<p>Reflecting on the claim we proposed, in this project we were trapped by the technical problems that we finally failed to carry user study.</p>
<hr>
<p>Thank you for reading this far!☺︎<br>In 2020, Oculus made it possible to track the player’s hand using cameras set on the headset. So the obstacles I confronted may not be a problem at all now.<br>Besides, you may wonder if this project is the same as what Disney Research did (here is the <a href="https://www.youtube.com/watch?v=Qxu_y8ABajQ" target="_blank" rel="noopener">link</a>).<br>I also think that research is very brilliant because it breaks the boundaries of virtual reality and the real world.<br>In our work, we failed to realize the tracking and rendering that they realized.<br>While reflecting on our purpose, we proposed some points that was not yet presented by their research - we propose to release users from the fear of failure in VR, and trigger users with more confidence to perform the task in the real world.</p>


<!-- Tags -->



<div class="tags">
    
</div>



<!-- Comments -->
<div>
    


</div>



            </div>
        </div>

        <!-- Footer -->
<footer id="footer">
    <div class="inner">
        <section>
            <h2>About</h2>
            <div>
                For further contacts, please check the links beside.
            </div>
        </section>
        <section>
            <h2>Follow</h2>
            <ul class="icons">
                
                
                    <li><a href="https://www.facebook.com/?ref=tn_tnmn" class="icon style2 fa-facebook" target="_blank"><span class="label">Facebook</span></a></li>
                
                
                
                
                    <li><a href="https://github.com/LeneJiang011" class="icon style2 fa-github" target="_blank"><span class="label">GitHub</span></a></li>
                
                
                
                
                
                    <li><a href="lenejiang011@gmail.com" class="icon style2 fa-envelope-o" target="_blank"><span class="label">Email</span></a></li>
                
                
            </ul>
        </section>
        <ul class="copyright">
            <li>&copy; Shixin Jiang. All rights reserved</li>
            <li>Design: <a href="http://html5up.net" target="_blank">HTML5 UP</a></li>
            <li>Hexo: <a href="http://www.codeblocq.com/" target="_blank">Jonathan Klughertz</a></li>
        </ul>
    </div>
</footer>
    </div>

    <!-- After footer scripts -->
    
<!-- jQuery -->
<script src="/js/jquery.min.js"></script>

<!-- skel -->
<script src="/js/skel.min.js"></script>

<!-- Custom Code -->
<script src="/js/util.js"></script>

<!--[if lte IE 8]>
<script src="/js/ie/respond.min.js"></script>
<![endif]-->

<!-- Custom Code -->
<script src="/js/main.js"></script>

<!-- Gallery -->
<script src="//cdn.rawgit.com/noelboss/featherlight/1.3.5/release/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments -->


</body>

</html>